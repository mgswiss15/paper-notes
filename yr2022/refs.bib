@unpublished{madhawa_graphnvp_2019,
    title = {{GraphNVP}: an Invertible Flow-based Model for Generating Molecular Graphs},
    url = {https://openreview.net/forum?id=ryxQ6T4YwB},
    shorttitle = {{GraphNVP}},
    abstract = {The first fully invertible flow-based generative model for molecular graphs is proposed.},
    author = {Madhawa, Kaushalya and Ishiguro, Katsuhiko and Nakago, Kosuke and Abe, Motoki},
    urldate = {2022-01-29},
    date = {2019-09-25},
    langid = {english},
    file = {Madhawa et al_2019_GraphNVP.pdf:/home/magda/Dropbox/Zot/Madhawa et al_2019_GraphNVP.pdf:application/pdf;Snapshot:/home/magda/Zotero fhws/storage/BYUCNH2V/forum.html:text/html},
}

@article{flam-shepherd_mpgvae_2021,
    title = {{MPGVAE}: improved generation of small organic molecules using message passing neural nets},
    volume = {2},
    issn = {2632-2153},
    url = {https://doi.org/10.1088/2632-2153/abf5b7},
    doi = {10.1088/2632-2153/abf5b7},
    shorttitle = {{MPGVAE}},
    abstract = {Graph generation is an extremely important task, as graphs are found throughout different areas of science and engineering. In this work, we focus on the modern equivalent of the Erdos–Rényi random graph model: the graph variational autoencoder ({GVAE}) (Simonovsky and Komodakis 2018 Int. Conf. on Artificial Neural Networks pp 412–22). This model assumes edges and nodes are independent in order to generate entire graphs at a time using a multi-layer perceptron decoder. As a result of these assumptions, {GVAE} has difficulty matching the training distribution and relies on an expensive graph matching procedure. We improve this class of models by building a message passing neural network into {GVAE}’s encoder and decoder. We demonstrate our model on the specific task of generating small organic molecules.},
    pages = {045010},
    number = {4},
    journaltitle = {Machine Learning: Science and Technology},
    shortjournal = {Mach. Learn.: Sci. Technol.},
    author = {Flam-Shepherd, Daniel and Wu, Tony C. and Aspuru-Guzik, Alan},
    urldate = {2022-01-29},
    date = {2021-07},
    langid = {english},
    note = {Publisher: {IOP} Publishing},
    file = {Flam-Shepherd et al_2021_MPGVAE.pdf:/home/magda/Dropbox/Zot/Flam-Shepherd et al_2021_MPGVAE.pdf:application/pdf},
}

@article{simonovsky_graphvae_2018,
    title = {{GraphVAE}: Towards Generation of Small Graphs Using Variational Autoencoders},
    url = {http://arxiv.org/abs/1802.03480},
    shorttitle = {{GraphVAE}},
    abstract = {Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.},
    journaltitle = {{arXiv}:1802.03480 [cs]},
    author = {Simonovsky, Martin and Komodakis, Nikos},
    urldate = {2021-12-10},
    date = {2018-02-09},
    eprinttype = {arxiv},
    eprint = {1802.03480},
    file = {Simonovsky_Komodakis_2018_GraphVAE.pdf:/home/magda/Dropbox/Zot/Simonovsky_Komodakis_2018_GraphVAE.pdf:application/pdf;arXiv.org Snapshot:/home/magda/Zotero fhws/storage/S3BB5YEE/1802.html:text/html},
}

@inproceedings{yao_federated_2022,
    title = {Federated multi-target domain adaptation},
    pages = {1424--1433},
    booktitle = {Proceedings of the {IEEE}/{CVF} Winter Conference on Applications of Computer Vision},
    author = {Yao, Chun-Han and Gong, Boqing and Qi, Hang and Cui, Yin and Zhu, Yukun and Yang, Ming-Hsuan},
    date = {2022},
    file = {Yao et al_2022_Federated multi-target domain adaptation.pdf:/home/magda/Dropbox/Zot/Yao et al_2022_Federated multi-target domain adaptation.pdf:application/pdf},
}

@inproceedings{ho_denoising_2020,
    title = {Denoising Diffusion Probabilistic Models},
    volume = {33},
    url = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
    pages = {6840--6851},
    booktitle = {Advances in Neural Information Processing Systems},
    publisher = {Curran Associates, Inc.},
    author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
    editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
    date = {2020},
    file = {Ho et al_2020_Denoising Diffusion Probabilistic Models.pdf:/home/magda/Dropbox/Zot/Ho et al_2020_Denoising Diffusion Probabilistic Models.pdf:application/pdf;Ho et al_2020_Denoising Diffusion Probabilistic Models.pdf:/home/magda/Dropbox/Zot/Ho et al_2020_Denoising Diffusion Probabilistic Models2.pdf:application/pdf},
}

@article{song_generative_2020,
    title = {Generative Modeling by Estimating Gradients of the Data Distribution},
    url = {http://arxiv.org/abs/1907.05600},
    abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to {GANs} on {MNIST}, {CelebA} and {CIFAR}-10 datasets, achieving a new state-of-the-art inception score of 8.87 on {CIFAR}-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
    journaltitle = {{arXiv}:1907.05600 [cs, stat]},
    author = {Song, Yang and Ermon, Stefano},
    urldate = {2022-01-27},
    date = {2020-10-10},
    eprinttype = {arxiv},
    eprint = {1907.05600},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv.org Snapshot:/home/magda/Zotero fhws/storage/N9TS2277/1907.html:text/html;Song_Ermon_2020_Generative Modeling by Estimating Gradients of the Data Distribution.pdf:/home/magda/Dropbox/Zot/Song_Ermon_2020_Generative Modeling by Estimating Gradients of the Data Distribution.pdf:application/pdf},
}

@article{welling_bayesian_2011,
    title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics},
    abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overﬁtting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a “sampling threshold” and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and {ICA} with natural gradients.},
    pages = {8},
    author = {Welling, Max and Teh, Yee Whye},
    date = {2011},
    langid = {english},
    keywords = {\_tablet},
    file = {Welling_Teh_Bayesian Learning via Stochastic Gradient Langevin Dynamics.pdf:/home/magda/Dropbox/Zot/Welling_Teh_Bayesian Learning via Stochastic Gradient Langevin Dynamics.pdf:application/pdf},
}

@article{hyvarinen_estimation_2005,
    title = {Estimation of Non-Normalized Statistical Models by Score Matching},
    abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data.},
    pages = {15},
    author = {Hyvarinen, Aapo},
    date = {2005},
    langid = {english},
    keywords = {skimmed},
    file = {Hyvarinen_Estimation of Non-Normalized Statistical Models by Score Matching.pdf:/home/magda/Dropbox/Zot/Hyvarinen_Estimation of Non-Normalized Statistical Models by Score Matching.pdf:application/pdf},
}

@article{vincent_connection_2011,
    title = {A Connection Between Score Matching and Denoising Autoencoders},
    volume = {23},
    issn = {0899-7667},
    doi = {10.1162/NECO_a_00142},
    abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
    pages = {1661--1674},
    number = {7},
    journaltitle = {Neural Computation},
    author = {Vincent, Pascal},
    date = {2011-07},
    note = {Conference Name: Neural Computation},
    keywords = {skimmed},
    file = {Vincent_2011_A Connection Between Score Matching and Denoising Autoencoders.pdf:/home/magda/Dropbox/Zot/Vincent_2011_A Connection Between Score Matching and Denoising Autoencoders.pdf:application/pdf;IEEE Xplore Abstract Record:/home/magda/Zotero/storage/A7GDR57A/6795935.html:text/html},
}